{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c372df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick-do/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "MONAI version: 1.5.0\n",
      "Numpy version: 1.26.1\n",
      "Pytorch version: 2.1.2+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: d388d1c6fec8cb3a0eebee5b5a0b9776ca59ca83\n",
      "MONAI __file__: /home/<username>/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 10.0.1\n",
      "Tensorboard version: 2.19.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.16.2+cu121\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 7.0.0\n",
      "pandas version: 2.3.1\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: 1.1.3\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14153c5",
   "metadata": {},
   "source": [
    "Define Hyperamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13aca52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "898"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import json, pathlib, datetime\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    mode = \"train\"\n",
    "    model_type: str = \"DDPM\"\n",
    "    image_size: int = 256  # the generated image resolution\n",
    "    num_img_channels: int = 1\n",
    "    train_batch_size: int = 32\n",
    "    eval_batch_size: int = 8  # how many images to sample during evaluation\n",
    "    num_epochs: int = 200\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_warmup_steps: int = 500\n",
    "    save_image_epochs: int = 20\n",
    "    save_model_epochs: int = 30\n",
    "    mixed_precision: str = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir: str = None\n",
    "    img_dir: str = \"data/img\"  # directory with training images\n",
    "    seg_dir: str = \"data/seg\"  # directory with training segmentations\n",
    "\n",
    "    # push_to_hub: bool = False  # whether to upload the saved model to the HF Hub\n",
    "    # hub_private_repo: bool = False\n",
    "    # overwrite_output_dir: bool = True  # overwrite the old model when re-running the notebook\n",
    "    seed: int = 0\n",
    "\n",
    "    # custom options\n",
    "    segmentation_guided: bool = True\n",
    "    segmentation_channel_mode: str = \"single\"\n",
    "    num_segmentation_classes: int = 2 # INCLUDING background\n",
    "    use_ablated_segmentations: bool = False\n",
    "    dataset: str = \"AVT_dongyang\"\n",
    "    resume_epoch: int = None\n",
    "\n",
    "    eval_sample_size: int = 100\n",
    "    eval_mask_removal: bool = True\n",
    "    eval_blank_mask: bool = True\n",
    "\n",
    "    #  EXPERIMENTAL/UNTESTED: classifier-free class guidance and image translation\n",
    "    class_conditional: bool = False\n",
    "    cfg_p_uncond: float = 0.2 # p_uncond in classifier-free guidance paper\n",
    "    cfg_weight: float = 0.3 # w in the paper\n",
    "    trans_noise_level: float = 0.5 # ratio of time step t to noise trans_start_images to total T before denoising in translation. e.g. value of 0.5 means t = 500 for default T = 1000.\n",
    "    use_cfg_for_eval_conditioning: bool = True  # whether to use classifier-free guidance for or just naive class conditioning for main sampling loop\n",
    "    cfg_maskguidance_condmodel_only: bool = True  # if using mask guidance AND cfg, only give mask to conditional network\n",
    "    # ^ this is because giving mask to both uncond and cond model make class guidance not work \n",
    "    # (see \"Classifier-free guidance resolution weighting.\" in ControlNet paper)\n",
    "\n",
    "# ---------- save ----------\n",
    "cfg = TrainConfig()\n",
    "\n",
    "cfg.output_dir = \"runs/AVT_dongyang\"\n",
    "pathlib.Path(cfg.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg.img_dir = \"data_dongyang/img\"\n",
    "cfg.seg_dir = \"data_dongyang/seg\"\n",
    "\n",
    "# Save the config to a JSON file\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = pathlib.Path(cfg.output_dir, ts, \"config.json\")\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "path.write_text(json.dumps(asdict(cfg), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import nrrd\n",
    "import numpy as np\n",
    "import psutil, shutil\n",
    "\n",
    "# ---------- tweak these two values ---------------------------------\n",
    "MIN_FREE_RAM_GB   = 1.0   # stop if < 1 GB RAM left\n",
    "MIN_FREE_DISK_GB  = 1.0   # stop if < 1 GB free on the caching partition\n",
    "CHECK_EVERY_N_SLICES = 50 # how often to poll resources\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def _enough_resources():\n",
    "    # --- RAM ---\n",
    "    avail_ram_gb = psutil.virtual_memory().available / 2**30\n",
    "    # --- disk (whatever partition holds ~/.cache) ---\n",
    "    cache_root   = os.path.expanduser(\"~/.cache\")\n",
    "    avail_disk_gb = shutil.disk_usage(cache_root).free / 2**30\n",
    "    return (\n",
    "        avail_ram_gb > MIN_FREE_RAM_GB\n",
    "        and avail_disk_gb > MIN_FREE_DISK_GB\n",
    "    ), avail_ram_gb, avail_disk_gb\n",
    "\n",
    "\n",
    "class NRRDDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 img_dir = None, \n",
    "                 seg_dir = None, \n",
    "                 split=\"train\",\n",
    "                 img_size=256,\n",
    "                 segmentation_guided=True,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.split = split\n",
    "        self.segmentation_guided = segmentation_guided\n",
    "        self.samples = []\n",
    "\n",
    "        seg_types = os.listdir(self.seg_dir)\n",
    "\n",
    "        #--- Transforms ---------------------------------------------------\n",
    "        img_tf = Compose([\n",
    "            # EnsureChannelFirst(),\n",
    "            Resize((img_size, img_size)),\n",
    "            ScaleIntensity(minv=-1.0, maxv=1.0),\n",
    "            ToTensor(),\n",
    "        ]) if img_dir is not None else None\n",
    "\n",
    "        seg_tf = Compose([\n",
    "            # EnsureChannelFirst(),\n",
    "            Resize((img_size, img_size), mode=\"nearest\"),\n",
    "            ToTensor(),\n",
    "        ]) if segmentation_guided else None\n",
    "\n",
    "        #--- Get Volume Paths ----------------------------------------\n",
    "        if img_dir is not None:\n",
    "            vol_paths = [os.path.join(img_dir, split, f) for f in os.listdir(os.path.join(img_dir, split)) if f.endswith('.nrrd')]\n",
    "        else:\n",
    "            vol_paths = [os.path.join(seg_dir, seg_type, split, f) for seg_type in seg_types for f in os.listdir(os.path.join(seg_dir, seg_type, split)) if f.endswith('.nrrd')]\n",
    "\n",
    "        # --- Pre‑load and Slice Volumes -------------------------------\n",
    "        for vol_path in vol_paths:\n",
    "            # read volume\n",
    "            vol_img = None\n",
    "            if img_dir is not None:\n",
    "                vol_img, _ = nrrd.read(vol_path)             # (H,W,D)\n",
    "\n",
    "            mask_vols = {}\n",
    "            if segmentation_guided:\n",
    "                for seg_type in seg_types:\n",
    "                    m_path = os.path.join(seg_dir, seg_type, split, os.path.basename(vol_path))\n",
    "                    mask_vols[seg_type], _ = nrrd.read(m_path)\n",
    "\n",
    "            depth = vol_img.shape[2] if img_dir else next(iter(mask_vols.values())).shape[2]\n",
    "\n",
    "            for z in range(depth):\n",
    "                record = {}\n",
    "\n",
    "                # image slice\n",
    "                if img_dir:\n",
    "                    img_slice = vol_img[:, :, z].astype(np.float32)\n",
    "                    img_slice = np.expand_dims(img_slice, axis=0)  # (1,H,W)\n",
    "                    img_slice = img_tf(img_slice)\n",
    "                    record[\"images\"] = img_slice\n",
    "\n",
    "                # mask slices\n",
    "                if segmentation_guided:\n",
    "                    for st in seg_types:\n",
    "                        m = mask_vols[st][:, :, z].astype(np.float32)\n",
    "                        m = np.expand_dims(m, axis=0) \n",
    "                        record[f\"seg_{st}\"] = seg_tf(m)\n",
    "\n",
    "                # filename\n",
    "                stem = os.path.splitext(os.path.basename(vol_path))[0]\n",
    "                record[\"image_filenames\"] = f\"{stem}_axial_{z:04d}\"\n",
    "\n",
    "                self.samples.append(record)\n",
    "                # slice_counter += 1\n",
    "\n",
    "                # # periodic resource check\n",
    "                # if slice_counter % CHECK_EVERY_N_SLICES == 0:\n",
    "                #     ok, ram, disk = _enough_resources()\n",
    "                #     if not ok:\n",
    "                #         print(\n",
    "                #             f\"[NRRDDataset] stopping preload:\"\n",
    "                #             f\" only {ram:.1f} GB RAM / {disk:.1f} GB disk free\"\n",
    "                #         )\n",
    "                #         return\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94209c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length : 1891 slices\n",
      "Keys           : dict_keys(['images', 'seg_all', 'image_filenames'])\n",
      "Image shape    : torch.Size([1, 256, 256])\n",
      "Mask shape     : torch.Size([1, 256, 256])\n",
      "Slice filename : D6_axial_0000\n",
      "Batch tensor keys : dict_keys(['images', 'seg_all', 'image_filenames'])\n",
      "Batch 'images'    : torch.Size([4, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "def make_loaders(\n",
    "    img_dir,\n",
    "    seg_dir,\n",
    "    img_size,\n",
    "    segmentation_guided,\n",
    "    batch_sizes,\n",
    "    num_workers=4,\n",
    "):\n",
    "    train_ds = NRRDDataset(\n",
    "        img_dir,\n",
    "        seg_dir,\n",
    "        split=\"train\",\n",
    "        img_size=img_size,\n",
    "        segmentation_guided=segmentation_guided,\n",
    "    )\n",
    "    val_ds = NRRDDataset(\n",
    "        img_dir,\n",
    "        seg_dir,\n",
    "        split=\"val\",\n",
    "        img_size=img_size,\n",
    "        segmentation_guided=segmentation_guided,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_sizes[\"train\"],\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_sizes[\"val\"],\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ------- create the raw dataset --------------------------------------\n",
    "ds = NRRDDataset(\n",
    "    img_dir=\"data_dongyang/img\",          # None if mask‑only\n",
    "    seg_dir=\"data_dongyang/seg\",           # required for segs\n",
    "    split=\"train\",\n",
    "    img_size=256,\n",
    "    segmentation_guided=True,       # False for image‑only\n",
    ")\n",
    "\n",
    "print(f\"Dataset length : {len(ds)} slices\")\n",
    "\n",
    "sample = ds[0]                      # any index < len(ds)\n",
    "print(\"Keys           :\", sample.keys())\n",
    "print(\"Image shape    :\", sample[\"images\"].shape)        # → (1, 256, 256)\n",
    "print(\"Mask shape     :\", sample[\"seg_all\"].shape)    # each → (1, 256, 256)\n",
    "print(\"Slice filename :\", sample[\"image_filenames\"])\n",
    "\n",
    "# ------- wrap in loaders ---------------------------------------------\n",
    "train_loader, val_loader = make_loaders(\n",
    "    img_dir=\"data_dongyang/img\",\n",
    "    seg_dir=\"data_dongyang/seg\",\n",
    "    img_size=256,\n",
    "    segmentation_guided=True,\n",
    "    batch_sizes={\"train\": 4, \"val\": 4},\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch tensor keys :\", batch.keys())\n",
    "print(\"Batch 'images'    :\", batch[\"images\"].shape)      # (B, 1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577824c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import diffusers\n",
    "\n",
    "from eval import evaluate, add_segmentations_to_noise, SegGuidedDDPMPipeline, SegGuidedDDIMPipeline\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, eval_dataloader, lr_scheduler, device='cuda'):\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # logging\n",
    "    run_name = '{}-{}-{}'.format(config.model_type.lower(), config.dataset, config.image_size)\n",
    "    if config.segmentation_guided:\n",
    "        run_name += \"-segguided\"\n",
    "    writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "    # for loading segs to condition on:\n",
    "    eval_dataloader = iter(eval_dataloader)\n",
    "\n",
    "    # Now you train the model\n",
    "    start_epoch = 0\n",
    "    if config.resume_epoch is not None:\n",
    "        start_epoch = config.resume_epoch\n",
    "\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            clean_images = clean_images.to(device)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            if config.segmentation_guided:\n",
    "                noisy_images = add_segmentations_to_noise(noisy_images, batch, config, device)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            if config.class_conditional:\n",
    "                class_labels = torch.ones(noisy_images.size(0)).long().to(device)\n",
    "                # classifier-free guidance\n",
    "                a = np.random.uniform()\n",
    "                if a <= config.cfg_p_uncond:\n",
    "                    class_labels = torch.zeros_like(class_labels).long()\n",
    "                noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
    "            else:\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # also train on target domain images if conditional\n",
    "            # (we don't have masks for this domain, so we can't do segmentation-guided; just use blank masks)\n",
    "            if config.class_conditional:\n",
    "                target_domain_images = batch['images_target']\n",
    "                target_domain_images = target_domain_images.to(device)\n",
    "\n",
    "                # Sample noise to add to the images\n",
    "                noise = torch.randn(target_domain_images.shape).to(target_domain_images.device)\n",
    "                bs = target_domain_images.shape[0]\n",
    "\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=target_domain_images.device).long()\n",
    "\n",
    "                # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_images = noise_scheduler.add_noise(target_domain_images, noise, timesteps)\n",
    "\n",
    "                if config.segmentation_guided:\n",
    "                    # no masks in target domain so just use blank masks\n",
    "                    noisy_images = torch.cat((noisy_images, torch.zeros_like(noisy_images)), dim=1)\n",
    "\n",
    "                # Predict the noise residual\n",
    "                class_labels = torch.full([noisy_images.size(0)], 2).long().to(device)\n",
    "                # classifier-free guidance\n",
    "                a = np.random.uniform()\n",
    "                if a <= config.cfg_p_uncond:\n",
    "                    class_labels = torch.zeros_like(class_labels).long()\n",
    "                noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
    "                loss_target_domain = F.mse_loss(noise_pred, noise)\n",
    "                loss_target_domain.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            if config.class_conditional:\n",
    "                logs = {\"loss\": loss.detach().item(), \"loss_target_domain\": loss_target_domain.detach().item(), \n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "                writer.add_scalar(\"loss_target_domain\", loss.detach().item(), global_step)\n",
    "            else: \n",
    "                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            writer.add_scalar(\"loss\", loss.detach().item(), global_step)\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if config.model_type == \"DDPM\":\n",
    "            if config.segmentation_guided:\n",
    "                pipeline = SegGuidedDDPMPipeline(\n",
    "                    unet=model.module, scheduler=noise_scheduler, eval_dataloader=eval_dataloader, external_config=config\n",
    "                    )\n",
    "            else:\n",
    "                if config.class_conditional:\n",
    "                    raise NotImplementedError(\"TODO: Conditional training not implemented for non-seg-guided DDPM\")\n",
    "                else:\n",
    "                    pipeline = diffusers.DDPMPipeline(unet=model.module, scheduler=noise_scheduler)\n",
    "        elif config.model_type == \"DDIM\":\n",
    "            if config.segmentation_guided:\n",
    "                pipeline = SegGuidedDDIMPipeline(\n",
    "                    unet=model.module, scheduler=noise_scheduler, eval_dataloader=eval_dataloader, external_config=config\n",
    "                    )\n",
    "            else:\n",
    "                if config.class_conditional:\n",
    "                    raise NotImplementedError(\"TODO: Conditional training not implemented for non-seg-guided DDIM\")\n",
    "                else:\n",
    "                    pipeline = diffusers.DDIMPipeline(unet=model.module, scheduler=noise_scheduler)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            if config.segmentation_guided:\n",
    "                seg_batch = next(eval_dataloader)\n",
    "                evaluate(config, epoch, pipeline, seg_batch)\n",
    "            else:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            pipeline.save_pretrained(config.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b510bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP1\n",
      "BP2\n",
      "BP3\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/473 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP4\n",
      "BP5\n",
      "BP6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 1/473 [00:09<1:17:32,  9.86s/it, loss=1.02, lr=2e-7, step=0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/473 [00:20<1:20:08, 10.21s/it, loss=1.04, lr=4e-7, step=1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 3/473 [00:31<1:23:49, 10.70s/it, loss=1.04, lr=6e-7, step=2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 4/473 [00:43<1:27:34, 11.20s/it, loss=1.06, lr=8e-7, step=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BP6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     66\u001b[39m     lr_scheduler = get_cosine_schedule_with_warmup(\n\u001b[32m     67\u001b[39m         optimizer=optimizer,\n\u001b[32m     68\u001b[39m         num_warmup_steps=cfg.lr_warmup_steps,\n\u001b[32m     69\u001b[39m         num_training_steps=(\u001b[38;5;28mlen\u001b[39m(train_loader) * cfg.num_epochs),\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    default eval behavior:\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    evaluate image generation or translation (if for conditional model, either evaluate naive class conditioning but not CFG,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m    has various options.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(config, model, noise_scheduler, optimizer, train_dataloader, eval_dataloader, lr_scheduler, device)\u001b[39m\n\u001b[32m     76\u001b[39m     noise_pred = model(noisy_images, timesteps, return_dict=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     77\u001b[39m loss = F.mse_loss(noise_pred, noise)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     81\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/torch/_tensor.py:483\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[32m    437\u001b[39m \n\u001b[32m    438\u001b[39m \u001b[33;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    480\u001b[39m \u001b[33;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    492\u001b[39m torch.autograd.backward(\n\u001b[32m    493\u001b[39m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs=inputs\n\u001b[32m    494\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/torch/overrides.py:1577\u001b[39m, in \u001b[36mhandle_torch_function\u001b[39m\u001b[34m(public_api, relevant_args, *args, **kwargs)\u001b[39m\n\u001b[32m   1571\u001b[39m     warnings.warn(\u001b[33m\"\u001b[39m\u001b[33mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1572\u001b[39m                   \u001b[33m\"\u001b[39m\u001b[33mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1573\u001b[39m                   \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m)\n\u001b[32m   1575\u001b[39m \u001b[38;5;66;03m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[32m   1576\u001b[39m \u001b[38;5;66;03m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1577\u001b[39m result = \u001b[43mtorch_func_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[32m   1580\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/monai/data/meta_tensor.py:283\u001b[39m, in \u001b[36mMetaTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    282\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/torch/_tensor.py:1386\u001b[39m, in \u001b[36mTensor.__torch_function__\u001b[39m\u001b[34m(cls, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m   1385\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _C.DisableTorchFunctionSubclass():\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[32m   1388\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    484\u001b[39m         Tensor.backward,\n\u001b[32m    485\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    490\u001b[39m         inputs=inputs,\n\u001b[32m    491\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    246\u001b[39m     retain_graph = create_graph\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "# custom imports\n",
    "# from training import train_loop\n",
    "from eval import evaluate_generation, evaluate_sample_many\n",
    "\n",
    "# define the model\n",
    "in_channels = cfg.num_img_channels\n",
    "if cfg.segmentation_guided:\n",
    "    assert cfg.num_segmentation_classes is not None\n",
    "    assert cfg.num_segmentation_classes > 1, \"must have at least 2 segmentation classes (INCLUDING background)\" \n",
    "    if cfg.segmentation_channel_mode == \"single\":\n",
    "        in_channels += 1\n",
    "    elif cfg.segmentation_channel_mode == \"multi\":\n",
    "        in_channels = len(os.listdir(cfg.seg_dir)) + in_channels\n",
    "\n",
    "model = diffusers.UNet2DModel(\n",
    "        sample_size=cfg.image_size,  # the target image resolution\n",
    "        in_channels=in_channels,  # the number of input channels, 3 for RGB images\n",
    "        out_channels=cfg.num_img_channels,  # the number of output channels\n",
    "        layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "mode = cfg.mode\n",
    "resume_epoch = cfg.resume_epoch\n",
    "model_type = cfg.model_type\n",
    "\n",
    "if (mode == \"train\" and resume_epoch is not None) or \"eval\" in mode:\n",
    "    if mode == \"train\":\n",
    "        print(\"resuming from model at training epoch {}\".format(resume_epoch))\n",
    "    elif \"eval\" in mode:\n",
    "        print(\"loading saved model...\")\n",
    "    model = model.from_pretrain0ed(os.path.join(cfg.output_dir, 'unet'), use_safetensors=True)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# define noise scheduler\n",
    "if model_type == \"DDPM\":\n",
    "    noise_scheduler = diffusers.DDPMScheduler(num_train_timesteps=1000)\n",
    "elif model_type == \"DDIM\":\n",
    "    noise_scheduler = diffusers.DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "if mode == \"train\":\n",
    "    # training setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=cfg.lr_warmup_steps,\n",
    "        num_training_steps=(len(train_loader) * cfg.num_epochs),\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    train_loop(\n",
    "        cfg, \n",
    "        model, \n",
    "        noise_scheduler, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        lr_scheduler, \n",
    "        device=device\n",
    "        )\n",
    "elif mode == \"eval\":\n",
    "    \"\"\"\n",
    "    default eval behavior:\n",
    "    evaluate image generation or translation (if for conditional model, either evaluate naive class conditioning but not CFG,\n",
    "    or with CFG),\n",
    "    possibly conditioned on masks.\n",
    "\n",
    "    has various options.\n",
    "    \"\"\"\n",
    "    evaluate_generation(\n",
    "        cfg, \n",
    "        model, \n",
    "        noise_scheduler,\n",
    "        val_loader, \n",
    "        eval_mask_removal=cfg.eval_mask_removal,\n",
    "        eval_blank_mask=cfg.eval_blank_mask,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "elif mode == \"eval_many\":\n",
    "    \"\"\"\n",
    "    generate many images and save them to a directory, saved individually\n",
    "    \"\"\"\n",
    "    evaluate_sample_many(\n",
    "        cfg.eval_sample_size,\n",
    "        cfg,\n",
    "        model,\n",
    "        noise_scheduler,\n",
    "        val_loader,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"mode \\\"{}\\\" not supported.\".format(mode))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
