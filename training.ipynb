{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c372df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "MONAI version: 1.5.0\n",
      "Numpy version: 1.26.1\n",
      "Pytorch version: 2.7.1+cu128\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: d388d1c6fec8cb3a0eebee5b5a0b9776ca59ca83\n",
      "MONAI __file__: c:\\Users\\<username>\\OneDrive\\Documents\\Notre Dame\\Medical Imaging\\synthetic-CT-slices\\.venv\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 10.0.1\n",
      "Tensorboard version: 2.19.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.22.1+cu128\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 7.0.0\n",
      "pandas version: 2.3.1\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: 1.1.3\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14153c5",
   "metadata": {},
   "source": [
    "Define Hyperamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13aca52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass, asdict\n",
    "import json, pathlib, datetime\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    mode = \"train\"\n",
    "    model_type: str = \"DDPM\"\n",
    "    image_size: int = 256  # the generated image resolution\n",
    "    num_img_channels: int = 1\n",
    "    train_batch_size: int = 32\n",
    "    eval_batch_size: int = 8  # how many images to sample during evaluation\n",
    "    num_epochs: int = 200\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_warmup_steps: int = 500\n",
    "    save_image_epochs: int = 20\n",
    "    save_model_epochs: int = 30\n",
    "    mixed_precision: str = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    output_dir: str = None\n",
    "    img_dir: str = \"data/img\"  # directory with training images\n",
    "    seg_dir: str = \"data/seg\"  # directory with training segmentations\n",
    "\n",
    "    # push_to_hub: bool = False  # whether to upload the saved model to the HF Hub\n",
    "    # hub_private_repo: bool = False\n",
    "    # overwrite_output_dir: bool = True  # overwrite the old model when re-running the notebook\n",
    "    seed: int = 0\n",
    "\n",
    "    # custom options\n",
    "    segmentation_guided: bool = True\n",
    "    segmentation_channel_mode: str = \"single\"\n",
    "    num_segmentation_classes: int = 2 # INCLUDING background\n",
    "    use_ablated_segmentations: bool = False\n",
    "    dataset: str = \"AVT_dongyang\"\n",
    "    resume_epoch: int = None\n",
    "\n",
    "    eval_sample_size: int = 100\n",
    "    eval_mask_removal: bool = True\n",
    "    eval_blank_mask: bool = True\n",
    "\n",
    "    #  EXPERIMENTAL/UNTESTED: classifier-free class guidance and image translation\n",
    "    class_conditional: bool = False\n",
    "    cfg_p_uncond: float = 0.2 # p_uncond in classifier-free guidance paper\n",
    "    cfg_weight: float = 0.3 # w in the paper\n",
    "    trans_noise_level: float = 0.5 # ratio of time step t to noise trans_start_images to total T before denoising in translation. e.g. value of 0.5 means t = 500 for default T = 1000.\n",
    "    use_cfg_for_eval_conditioning: bool = True  # whether to use classifier-free guidance for or just naive class conditioning for main sampling loop\n",
    "    cfg_maskguidance_condmodel_only: bool = True  # if using mask guidance AND cfg, only give mask to conditional network\n",
    "    # ^ this is because giving mask to both uncond and cond model make class guidance not work \n",
    "    # (see \"Classifier-free guidance resolution weighting.\" in ControlNet paper)\n",
    "\n",
    "# ---------- save ----------\n",
    "cfg = TrainConfig()\n",
    "\n",
    "cfg.output_dir = \"runs/AVT_dongyang\"\n",
    "pathlib.Path(cfg.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cfg.img_dir = \"data_dongyang/img\"\n",
    "cfg.seg_dir = \"data_dongyang/seg\"\n",
    "\n",
    "# Save the config to a JSON file\n",
    "ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "path = pathlib.Path(cfg.output_dir, ts, \"config.json\")\n",
    "path.parent.mkdir(parents=True, exist_ok=True)\n",
    "path.write_text(json.dumps(asdict(cfg), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2e411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import nrrd\n",
    "import numpy as np\n",
    "import psutil, shutil\n",
    "\n",
    "# ---------- tweak these two values ---------------------------------\n",
    "MIN_FREE_RAM_GB   = 1.0   # stop if < 1 GB RAM left\n",
    "MIN_FREE_DISK_GB  = 1.0   # stop if < 1 GB free on the caching partition\n",
    "CHECK_EVERY_N_SLICES = 50 # how often to poll resources\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def _enough_resources():\n",
    "    # --- RAM ---\n",
    "    avail_ram_gb = psutil.virtual_memory().available / 2**30\n",
    "    # --- disk (whatever partition holds ~/.cache) ---\n",
    "    cache_root   = os.path.expanduser(\"~/.cache\")\n",
    "    avail_disk_gb = shutil.disk_usage(cache_root).free / 2**30\n",
    "    return (\n",
    "        avail_ram_gb > MIN_FREE_RAM_GB\n",
    "        and avail_disk_gb > MIN_FREE_DISK_GB\n",
    "    ), avail_ram_gb, avail_disk_gb\n",
    "\n",
    "\n",
    "class NRRDDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 img_dir = None, \n",
    "                 seg_dir = None, \n",
    "                 split=\"train\",\n",
    "                 img_size=256,\n",
    "                 segmentation_guided=True,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.split = split\n",
    "        self.segmentation_guided = segmentation_guided\n",
    "        self.samples = []\n",
    "\n",
    "        seg_types = os.listdir(self.seg_dir)\n",
    "\n",
    "        #--- Transforms ---------------------------------------------------\n",
    "        img_tf = Compose([\n",
    "            # EnsureChannelFirst(),\n",
    "            Resize((img_size, img_size)),\n",
    "            ScaleIntensity(minv=-1.0, maxv=1.0),\n",
    "            ToTensor(),\n",
    "        ]) if img_dir is not None else None\n",
    "\n",
    "        seg_tf = Compose([\n",
    "            # EnsureChannelFirst(),\n",
    "            Resize((img_size, img_size), mode=\"nearest\"),\n",
    "            ToTensor(),\n",
    "        ]) if segmentation_guided else None\n",
    "\n",
    "        #--- Get Volume Paths ----------------------------------------\n",
    "        if img_dir is not None:\n",
    "            vol_paths = [os.path.join(img_dir, split, f) for f in os.listdir(os.path.join(img_dir, split)) if f.endswith('.nrrd')]\n",
    "        else:\n",
    "            vol_paths = [os.path.join(seg_dir, seg_type, split, f) for seg_type in seg_types for f in os.listdir(os.path.join(seg_dir, seg_type, split)) if f.endswith('.nrrd')]\n",
    "\n",
    "        # --- Pre‑load and Slice Volumes -------------------------------\n",
    "        for vol_path in vol_paths:\n",
    "            # read volume\n",
    "            vol_img = None\n",
    "            if img_dir is not None:\n",
    "                vol_img, _ = nrrd.read(vol_path)             # (H,W,D)\n",
    "\n",
    "            mask_vols = {}\n",
    "            if segmentation_guided:\n",
    "                for seg_type in seg_types:\n",
    "                    m_path = os.path.join(seg_dir, seg_type, split, os.path.basename(vol_path))\n",
    "                    mask_vols[seg_type], _ = nrrd.read(m_path)\n",
    "\n",
    "            depth = vol_img.shape[2] if img_dir else next(iter(mask_vols.values())).shape[2]\n",
    "\n",
    "            for z in range(depth):\n",
    "                record = {}\n",
    "\n",
    "                # image slice\n",
    "                if img_dir:\n",
    "                    img_slice = vol_img[:, :, z].astype(np.float32)\n",
    "                    img_slice = np.expand_dims(img_slice, axis=0)  # (1,H,W)\n",
    "                    img_slice = img_tf(img_slice)\n",
    "                    record[\"images\"] = img_slice\n",
    "\n",
    "                # mask slices\n",
    "                if segmentation_guided:\n",
    "                    for st in seg_types:\n",
    "                        m = mask_vols[st][:, :, z].astype(np.float32)\n",
    "                        m = np.expand_dims(m, axis=0) \n",
    "                        record[f\"seg_{st}\"] = seg_tf(m)\n",
    "\n",
    "                # filename\n",
    "                stem = os.path.splitext(os.path.basename(vol_path))[0]\n",
    "                record[\"image_filenames\"] = f\"{stem}_axial_{z:04d}\"\n",
    "\n",
    "                self.samples.append(record)\n",
    "                # slice_counter += 1\n",
    "\n",
    "                # # periodic resource check\n",
    "                # if slice_counter % CHECK_EVERY_N_SLICES == 0:\n",
    "                #     ok, ram, disk = _enough_resources()\n",
    "                #     if not ok:\n",
    "                #         print(\n",
    "                #             f\"[NRRDDataset] stopping preload:\"\n",
    "                #             f\" only {ram:.1f} GB RAM / {disk:.1f} GB disk free\"\n",
    "                #         )\n",
    "                #         return\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94209c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length : 1879 slices\n",
      "Keys           : dict_keys(['images', 'seg_all', 'image_filenames'])\n",
      "Image shape    : torch.Size([1, 256, 256])\n",
      "Mask shape     : torch.Size([1, 256, 256])\n",
      "Slice filename : D10_axial_0000\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def make_loaders(\n",
    "    img_dir,\n",
    "    seg_dir,\n",
    "    img_size,\n",
    "    segmentation_guided,\n",
    "    batch_sizes,\n",
    "    num_workers=1,\n",
    "):\n",
    "    train_ds = NRRDDataset(\n",
    "        img_dir,\n",
    "        seg_dir,\n",
    "        split=\"train\",\n",
    "        img_size=img_size,\n",
    "        segmentation_guided=segmentation_guided,\n",
    "    )\n",
    "\n",
    "    \n",
    "    val_ds = NRRDDataset(\n",
    "        img_dir,\n",
    "        seg_dir,\n",
    "        split=\"val\",\n",
    "        img_size=img_size,\n",
    "        segmentation_guided=segmentation_guided,\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_sizes[\"train\"],\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_sizes[\"val\"],\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ------- create the raw dataset --------------------------------------\n",
    "ds = NRRDDataset(\n",
    "    img_dir=\"data_dongyang/img\",          # None if mask‑only\n",
    "    seg_dir=\"data_dongyang/seg\",           # required for segs\n",
    "    split=\"train\",\n",
    "    img_size=256,\n",
    "    segmentation_guided=True,       # False for image‑only\n",
    ")\n",
    "\n",
    "print(f\"Dataset length : {len(ds)} slices\")\n",
    "\n",
    "sample = ds[0]\n",
    "print(\"Keys           :\", sample.keys())\n",
    "print(\"Image shape    :\", sample[\"images\"].shape)        # → (1, 256, 256)\n",
    "print(\"Mask shape     :\", sample[\"seg_all\"].shape)    # each → (1, 256, 256)\n",
    "print(\"Slice filename :\", sample[\"image_filenames\"])\n",
    "\n",
    "# ------- wrap in loaders ---------------------------------------------\n",
    "train_loader, val_loader = make_loaders(\n",
    "    img_dir=\"data_dongyang/img\",\n",
    "    seg_dir=\"data_dongyang/seg\",\n",
    "    img_size=cfg.image_size,\n",
    "    segmentation_guided=True,\n",
    "    batch_sizes={\"train\": cfg.train_batch_size, \"val\": cfg.eval_batch_size},\n",
    ")\n",
    "print(4)\n",
    "batch = next(iter(train_loader))\n",
    "print(5)\n",
    "print(\"Batch tensor keys :\", batch.keys())\n",
    "print(\"Batch 'images'    :\", batch[\"images\"].shape)      # (B, 1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577824c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import diffusers\n",
    "\n",
    "from eval import evaluate, add_segmentations_to_noise, SegGuidedDDPMPipeline, SegGuidedDDIMPipeline\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, eval_dataloader, lr_scheduler, device='cuda'):\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # logging\n",
    "    # run_name = '{}-{}-{}'.format(config.model_type.lower(), config.dataset, config.image_size)\n",
    "    # if config.segmentation_guided:\n",
    "    #     run_name += \"-segguided\"\n",
    "    # writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "    # for loading segs to condition on:\n",
    "    eval_dataloader = iter(eval_dataloader)\n",
    "\n",
    "    # Now you train the model\n",
    "    start_epoch = 0\n",
    "    if config.resume_epoch is not None:\n",
    "        start_epoch = config.resume_epoch\n",
    "\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{config.num_epochs}\")\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            clean_images = clean_images.to(device)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            if config.segmentation_guided:\n",
    "                noisy_images = add_segmentations_to_noise(noisy_images, batch, config, device)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            if config.class_conditional:\n",
    "                class_labels = torch.ones(noisy_images.size(0)).long().to(device)\n",
    "                # classifier-free guidance\n",
    "                a = np.random.uniform()\n",
    "                if a <= config.cfg_p_uncond:\n",
    "                    class_labels = torch.zeros_like(class_labels).long()\n",
    "                noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
    "            else:\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # also train on target domain images if conditional\n",
    "            # (we don't have masks for this domain, so we can't do segmentation-guided; just use blank masks)\n",
    "            if config.class_conditional:\n",
    "                target_domain_images = batch['images_target']\n",
    "                target_domain_images = target_domain_images.to(device)\n",
    "\n",
    "                # Sample noise to add to the images\n",
    "                noise = torch.randn(target_domain_images.shape).to(target_domain_images.device)\n",
    "                bs = target_domain_images.shape[0]\n",
    "\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=target_domain_images.device).long()\n",
    "\n",
    "                # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_images = noise_scheduler.add_noise(target_domain_images, noise, timesteps)\n",
    "\n",
    "                if config.segmentation_guided:\n",
    "                    # no masks in target domain so just use blank masks\n",
    "                    noisy_images = torch.cat((noisy_images, torch.zeros_like(noisy_images)), dim=1)\n",
    "\n",
    "                # Predict the noise residual\n",
    "                class_labels = torch.full([noisy_images.size(0)], 2).long().to(device)\n",
    "                # classifier-free guidance\n",
    "                a = np.random.uniform()\n",
    "                if a <= config.cfg_p_uncond:\n",
    "                    class_labels = torch.zeros_like(class_labels).long()\n",
    "                noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
    "                loss_target_domain = F.mse_loss(noise_pred, noise)\n",
    "                loss_target_domain.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            if config.class_conditional:\n",
    "                logs = {\"loss\": loss.detach().item(), \"loss_target_domain\": loss_target_domain.detach().item(), \n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "                # writer.add_scalar(\"loss_target_domain\", loss.detach().item(), global_step)\n",
    "            else: \n",
    "                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            # writer.add_scalar(\"loss\", loss.detach().item(), global_step)\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if config.model_type == \"DDPM\":\n",
    "            if config.segmentation_guided:\n",
    "                pipeline = SegGuidedDDPMPipeline(\n",
    "                    unet=model.module, scheduler=noise_scheduler, eval_dataloader=eval_dataloader, external_config=config\n",
    "                    )\n",
    "            else:\n",
    "                if config.class_conditional:\n",
    "                    raise NotImplementedError(\"TODO: Conditional training not implemented for non-seg-guided DDPM\")\n",
    "                else:\n",
    "                    pipeline = diffusers.DDPMPipeline(unet=model.module, scheduler=noise_scheduler)\n",
    "        elif config.model_type == \"DDIM\":\n",
    "            if config.segmentation_guided:\n",
    "                pipeline = SegGuidedDDIMPipeline(\n",
    "                    unet=model.module, scheduler=noise_scheduler, eval_dataloader=eval_dataloader, external_config=config\n",
    "                    )\n",
    "            else:\n",
    "                if config.class_conditional:\n",
    "                    raise NotImplementedError(\"TODO: Conditional training not implemented for non-seg-guided DDIM\")\n",
    "                else:\n",
    "                    pipeline = diffusers.DDIMPipeline(unet=model.module, scheduler=noise_scheduler)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            if config.segmentation_guided:\n",
    "                seg_batch = next(eval_dataloader)\n",
    "                evaluate(config, epoch, pipeline, seg_batch)\n",
    "            else:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            pipeline.save_pretrained(config.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510bb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     66\u001b[39m     lr_scheduler = get_cosine_schedule_with_warmup(\n\u001b[32m     67\u001b[39m         optimizer=optimizer,\n\u001b[32m     68\u001b[39m         num_warmup_steps=cfg.lr_warmup_steps,\n\u001b[32m     69\u001b[39m         num_training_steps=(\u001b[38;5;28mlen\u001b[39m(train_loader) * cfg.num_epochs),\n\u001b[32m     70\u001b[39m     )\n\u001b[32m     72\u001b[39m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnoise_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     84\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    default eval behavior:\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[33;03m    evaluate image generation or translation (if for conditional model, either evaluate naive class conditioning but not CFG,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m    has various options.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(config, model, noise_scheduler, optimizer, train_dataloader, eval_dataloader, lr_scheduler, device)\u001b[39m\n\u001b[32m     67\u001b[39m     noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     noise_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     70\u001b[39m loss = F.mse_loss(noise_pred, noise)\n\u001b[32m     71\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:192\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m     module_kwargs = ({},)\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.device_ids) == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m    194\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.parallel_apply(replicas, inputs, module_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d.py:337\u001b[39m, in \u001b[36mUNet2DModel.forward\u001b[39m\u001b[34m(self, sample, timestep, class_labels, return_dict)\u001b[39m\n\u001b[32m    335\u001b[39m         sample, skip_sample = upsample_block(sample, res_samples, emb, skip_sample)\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m         sample = \u001b[43mupsample_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;66;03m# 6. post-process\u001b[39;00m\n\u001b[32m    340\u001b[39m sample = \u001b[38;5;28mself\u001b[39m.conv_norm_out(sample)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:2566\u001b[39m, in \u001b[36mUpBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, *args, **kwargs)\u001b[39m\n\u001b[32m   2564\u001b[39m         hidden_states = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(resnet, hidden_states, temb)\n\u001b[32m   2565\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2566\u001b[39m         hidden_states = \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2569\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m upsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsamplers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\diffusers\\models\\resnet.py:327\u001b[39m, in \u001b[36mResnetBlock2D.forward\u001b[39m\u001b[34m(self, input_tensor, temb, *args, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m     deprecate(\u001b[33m\"\u001b[39m\u001b[33mscale\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1.0.0\u001b[39m\u001b[33m\"\u001b[39m, deprecation_message)\n\u001b[32m    325\u001b[39m hidden_states = input_tensor\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.nonlinearity(hidden_states)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.upsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;66;03m# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:313\u001b[39m, in \u001b[36mGroupNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\patri\\miniconda3\\envs\\test\\Lib\\site-packages\\torch\\nn\\functional.py:2965\u001b[39m, in \u001b[36mgroup_norm\u001b[39m\u001b[34m(input, num_groups, weight, bias, eps)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2959\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m _verify_batch_size(\n\u001b[32m   2962\u001b[39m     [\u001b[38;5;28minput\u001b[39m.size(\u001b[32m0\u001b[39m) * \u001b[38;5;28minput\u001b[39m.size(\u001b[32m1\u001b[39m) // num_groups, num_groups]\n\u001b[32m   2963\u001b[39m     + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m.size()[\u001b[32m2\u001b[39m:])\n\u001b[32m   2964\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import diffusers\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "\n",
    "# custom imports\n",
    "# from training import train_loop\n",
    "from eval import evaluate_generation, evaluate_sample_many\n",
    "\n",
    "# define the model\n",
    "in_channels = cfg.num_img_channels\n",
    "if cfg.segmentation_guided:\n",
    "    assert cfg.num_segmentation_classes is not None\n",
    "    assert cfg.num_segmentation_classes > 1, \"must have at least 2 segmentation classes (INCLUDING background)\" \n",
    "    if cfg.segmentation_channel_mode == \"single\":\n",
    "        in_channels += 1\n",
    "    elif cfg.segmentation_channel_mode == \"multi\":\n",
    "        in_channels = len(os.listdir(cfg.seg_dir)) + in_channels\n",
    "\n",
    "model = diffusers.UNet2DModel(\n",
    "        sample_size=cfg.image_size,  # the target image resolution\n",
    "        in_channels=in_channels,  # the number of input channels, 3 for RGB images\n",
    "        out_channels=cfg.num_img_channels,  # the number of output channels\n",
    "        layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",\n",
    "            \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"DownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "mode = cfg.mode\n",
    "resume_epoch = cfg.resume_epoch\n",
    "model_type = cfg.model_type\n",
    "\n",
    "if (mode == \"train\" and resume_epoch is not None) or \"eval\" in mode:\n",
    "    if mode == \"train\":\n",
    "        print(\"resuming from model at training epoch {}\".format(resume_epoch))\n",
    "    elif \"eval\" in mode:\n",
    "        print(\"loading saved model...\")\n",
    "    model = model.from_pretrain0ed(os.path.join(cfg.output_dir, 'unet'), use_safetensors=True)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# define noise scheduler\n",
    "if model_type == \"DDPM\":\n",
    "    noise_scheduler = diffusers.DDPMScheduler(num_train_timesteps=1000)\n",
    "elif model_type == \"DDIM\":\n",
    "    noise_scheduler = diffusers.DDIMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "if mode == \"train\":\n",
    "    # training setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.learning_rate)\n",
    "    lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=cfg.lr_warmup_steps,\n",
    "        num_training_steps=(len(train_loader) * cfg.num_epochs),\n",
    "    )\n",
    "\n",
    "    # train\n",
    "    train_loop(\n",
    "        cfg, \n",
    "        model, \n",
    "        noise_scheduler, \n",
    "        optimizer, \n",
    "        train_loader, \n",
    "        val_loader, \n",
    "        lr_scheduler, \n",
    "        device=device\n",
    "        )\n",
    "elif mode == \"eval\":\n",
    "    \"\"\"\n",
    "    default eval behavior:\n",
    "    evaluate image generation or translation (if for conditional model, either evaluate naive class conditioning but not CFG,\n",
    "    or with CFG),\n",
    "    possibly conditioned on masks.\n",
    "\n",
    "    has various options.\n",
    "    \"\"\"\n",
    "    evaluate_generation(\n",
    "        cfg, \n",
    "        model, \n",
    "        noise_scheduler,\n",
    "        val_loader, \n",
    "        eval_mask_removal=cfg.eval_mask_removal,\n",
    "        eval_blank_mask=cfg.eval_blank_mask,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "elif mode == \"eval_many\":\n",
    "    \"\"\"\n",
    "    generate many images and save them to a directory, saved individually\n",
    "    \"\"\"\n",
    "    evaluate_sample_many(\n",
    "        cfg.eval_sample_size,\n",
    "        cfg,\n",
    "        model,\n",
    "        noise_scheduler,\n",
    "        val_loader,\n",
    "        device=device\n",
    "        )\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"mode \\\"{}\\\" not supported.\".format(mode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6048354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'sm_80', 'sm_86', 'sm_90', 'sm_100', 'sm_120']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12, 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, subprocess, re, platform\n",
    "\n",
    "print(torch.cuda.get_arch_list())\n",
    "torch.cuda.get_device_capability(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
