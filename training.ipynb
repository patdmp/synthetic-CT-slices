{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c372df6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AddChannel' from 'monai.transforms' (/home/patrick-do/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/monai/transforms/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m print_config\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, ImageDataset\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmonai\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     EnsureChannelFirst,\n\u001b[32m     16\u001b[39m     AddChannel,\n\u001b[32m     17\u001b[39m     Compose,\n\u001b[32m     18\u001b[39m     Resize,\n\u001b[32m     19\u001b[39m     ScaleIntensity,\n\u001b[32m     20\u001b[39m     ToTensor,\n\u001b[32m     21\u001b[39m )\n\u001b[32m     23\u001b[39m pin_memory = torch.cuda.is_available()\n\u001b[32m     24\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AddChannel' from 'monai.transforms' (/home/patrick-do/Documents/Projects/synthetic-CT-slices/.venv/lib/python3.11/site-packages/monai/transforms/__init__.py)"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, ImageDataset\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirst,\n",
    "    Compose,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device:', device)\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14153c5",
   "metadata": {},
   "source": [
    "Define Hyperamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13aca52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'data_dongyang'\n",
    "# model_type = 'DDIM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2e411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import nrrd\n",
    "import numpy as np\n",
    "\n",
    "class NRRDDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 img_dir = None, \n",
    "                 seg_dir = None, \n",
    "                 split=\"train\",\n",
    "                 img_size=256,\n",
    "                 segmentation_guided=True,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.img_dir = img_dir\n",
    "        self.seg_dir = seg_dir\n",
    "        self.split = split\n",
    "        self.segmentation_guided = segmentation_guided\n",
    "        self.samples = []\n",
    "\n",
    "        seg_types = os.listdir(self.seg_dir)\n",
    "\n",
    "        #--- Transforms ---------------------------------------------------\n",
    "        img_tf = Compose([\n",
    "            # EnsureChannelFirst(),\n",
    "            Resize((img_size, img_size)),\n",
    "            ScaleIntensity(minv=-1.0, maxv=1.0),\n",
    "            ToTensor(),\n",
    "        ]) if img_dir is not None else None\n",
    "\n",
    "        seg_tf = Compose([\n",
    "            # EnsureChannelFirst(),\n",
    "            Resize((img_size, img_size), mode=\"nearest\"),\n",
    "            ToTensor(),\n",
    "        ]) if segmentation_guided else None\n",
    "\n",
    "        #--- Get Volume Paths ----------------------------------------\n",
    "        if img_dir is not None:\n",
    "            vol_paths = [os.path.join(img_dir, split, f) for f in os.listdir(os.path.join(img_dir, split)) if f.endswith('.nrrd')]\n",
    "        else:\n",
    "            vol_paths = [os.path.join(seg_dir, seg_type, split, f) for seg_type in seg_types for f in os.listdir(os.path.join(seg_dir, seg_type, split)) if f.endswith('.nrrd')]\n",
    "\n",
    "        #--- Pre-load and Slice Volumes -------------------------------\n",
    "        for vol_path in vol_paths:\n",
    "            vol_img = None\n",
    "            if img_dir is not None:\n",
    "                vol_img, _ = nrrd.read(vol_path)           # ndarray (H,W,D)\n",
    "\n",
    "            mask_vols = {}\n",
    "            if segmentation_guided:\n",
    "                for seg_type in seg_types:\n",
    "                    m_path = os.path.join(seg_dir, seg_type, split, os.path.basename(vol_path))\n",
    "                    mask_vols[seg_type], _ = nrrd.read(m_path)\n",
    "\n",
    "            depth = (\n",
    "                vol_img.shape[2] if img_dir is not None else next(iter(mask_vols.values())).shape[2]\n",
    "            )\n",
    "\n",
    "            for z in range(depth):\n",
    "                record = {}\n",
    "\n",
    "                # --- image slice -------------------------------------------\n",
    "                if img_dir is not None:\n",
    "                    img_slice = vol_img[:, :, z].astype(np.float32)\n",
    "                    # img_slice = np.expand_dims(img_slice, axis=0) \n",
    "                    img_slice = img_tf(img_slice)          # (1,H,W)\n",
    "                    record[\"images\"] = img_slice\n",
    "\n",
    "                # --- mask slices -------------------------------------------\n",
    "                if segmentation_guided:\n",
    "                    for st in seg_types:\n",
    "                        m = mask_vols[st][:, :, z].astype(np.float32)\n",
    "                        # m = np.expand_dims(m, axis=0) \n",
    "                        record[f\"seg_{st}\"] = seg_tf(m)   # (1,H,W)\n",
    "\n",
    "                # --- filename metadata ------------------------------------\n",
    "                stem = os.path.splitext(os.path.basename(vol_path))[0]\n",
    "                record[\"image_filenames\"] = f\"{stem}_axial_{z:04d}\"\n",
    "\n",
    "                self.samples.append(record)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94209c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(\n",
    "    img_dir,\n",
    "    seg_dir,\n",
    "    img_size,\n",
    "    segmentation_guided,\n",
    "    batch_sizes,\n",
    "    num_workers=4,\n",
    "):\n",
    "    train_ds = NRRDDataset(\n",
    "        img_dir,\n",
    "        seg_dir,\n",
    "        split=\"train\",\n",
    "        img_size=img_size,\n",
    "        segmentation_guided=segmentation_guided,\n",
    "    )\n",
    "    val_ds = NRRDDataset(\n",
    "        img_dir,\n",
    "        seg_dir,\n",
    "        split=\"val\",\n",
    "        img_size=img_size,\n",
    "        segmentation_guided=segmentation_guided,\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_sizes[\"train\"],\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_sizes[\"val\"],\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ------- create the raw dataset --------------------------------------\n",
    "ds = NRRDDataset(\n",
    "    img_dir=\"data_dongyang/img\",          # None if mask‑only\n",
    "    seg_dir=\"data_dongyang/seg\",           # required for segs\n",
    "    split=\"train\",\n",
    "    img_size=256,\n",
    "    segmentation_guided=True,       # False for image‑only\n",
    ")\n",
    "\n",
    "print(f\"Dataset length : {len(ds)} slices\")\n",
    "\n",
    "sample = ds[0]                      # any index < len(ds)\n",
    "print(\"Keys           :\", sample.keys())\n",
    "print(\"Image shape    :\", sample[\"images\"].shape)        # → (1, 256, 256)\n",
    "print(\"Mask shape     :\", sample[\"seg_all\"].shape)    # each → (1, 256, 256)\n",
    "print(\"Slice filename :\", sample[\"image_filenames\"])\n",
    "\n",
    "# ------- wrap in loaders ---------------------------------------------\n",
    "train_loader, val_loader = make_loaders(\n",
    "    img_dir=\"data_dongyang/img\",\n",
    "    seg_dir=\"data_dongyqang/seg\",\n",
    "    img_size=256,\n",
    "    segmentation_guided=True,\n",
    "    batch_sizes={\"train\": 4, \"val\": 4},\n",
    ")\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch tensor keys :\", batch.keys())\n",
    "print(\"Batch 'images'    :\", batch[\"images\"].shape)      # (B, 1, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b510bb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = diffusers.UNet2DModel(\n",
    "#         sample_size=config.image_size,  # the target image resolution\n",
    "#         in_channels=in_channels,  # the number of input channels, 3 for RGB images\n",
    "#         out_channels=num_img_channels,  # the number of output channels\n",
    "#         layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "#         block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n",
    "#         down_block_types=(\n",
    "#             \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "#             \"DownBlock2D\",\n",
    "#             \"DownBlock2D\",\n",
    "#             \"DownBlock2D\",\n",
    "#             \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "#             \"DownBlock2D\",\n",
    "#         ),\n",
    "#         up_block_types=(\n",
    "#             \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "#             \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "#             \"UpBlock2D\",\n",
    "#             \"UpBlock2D\",\n",
    "#             \"UpBlock2D\",\n",
    "#             \"UpBlock2D\"\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "# model = nn.DataParallel(model)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a17e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define noise scheduler\n",
    "#     if model_type == \"DDPM\":\n",
    "#         noise_scheduler = diffusers.DDPMScheduler(num_train_timesteps=1000)\n",
    "#     elif model_type == \"DDIM\":\n",
    "#         noise_scheduler = diffusers.DDIMScheduler(num_train_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba872323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import diffusers\n",
    "\n",
    "from eval import evaluate, add_segmentations_to_noise, SegGuidedDDPMPipeline, SegGuidedDDIMPipeline\n",
    "\n",
    "def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, eval_dataloader, lr_scheduler, device='cuda'):\n",
    "    # Prepare everything\n",
    "    # There is no specific order to remember, you just need to unpack the\n",
    "    # objects in the same order you gave them to the prepare method.\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    # logging\n",
    "    run_name = '{}-{}-{}'.format(config.model_type.lower(), config.dataset, config.image_size)\n",
    "    if config.segmentation_guided:\n",
    "        run_name += \"-segguided\"\n",
    "    writer = SummaryWriter(comment=run_name)\n",
    "\n",
    "    # for loading segs to condition on:\n",
    "    eval_dataloader = iter(eval_dataloader)\n",
    "\n",
    "    # Now you train the model\n",
    "    start_epoch = 0\n",
    "    if config.resume_epoch is not None:\n",
    "        start_epoch = config.resume_epoch\n",
    "\n",
    "    for epoch in range(start_epoch, config.num_epochs):\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['images']\n",
    "            clean_images = clean_images.to(device)\n",
    "\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape).to(clean_images.device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "\n",
    "            if config.segmentation_guided:\n",
    "                noisy_images = add_segmentations_to_noise(noisy_images, batch, config, device)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            if config.class_conditional:\n",
    "                class_labels = torch.ones(noisy_images.size(0)).long().to(device)\n",
    "                # classifier-free guidance\n",
    "                a = np.random.uniform()\n",
    "                if a <= config.cfg_p_uncond:\n",
    "                    class_labels = torch.zeros_like(class_labels).long()\n",
    "                noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
    "            else:\n",
    "                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # also train on target domain images if conditional\n",
    "            # (we don't have masks for this domain, so we can't do segmentation-guided; just use blank masks)\n",
    "            if config.class_conditional:\n",
    "                target_domain_images = batch['images_target']\n",
    "                target_domain_images = target_domain_images.to(device)\n",
    "\n",
    "                # Sample noise to add to the images\n",
    "                noise = torch.randn(target_domain_images.shape).to(target_domain_images.device)\n",
    "                bs = target_domain_images.shape[0]\n",
    "\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bs,), device=target_domain_images.device).long()\n",
    "\n",
    "                # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_images = noise_scheduler.add_noise(target_domain_images, noise, timesteps)\n",
    "\n",
    "                if config.segmentation_guided:\n",
    "                    # no masks in target domain so just use blank masks\n",
    "                    noisy_images = torch.cat((noisy_images, torch.zeros_like(noisy_images)), dim=1)\n",
    "\n",
    "                # Predict the noise residual\n",
    "                class_labels = torch.full([noisy_images.size(0)], 2).long().to(device)\n",
    "                # classifier-free guidance\n",
    "                a = np.random.uniform()\n",
    "                if a <= config.cfg_p_uncond:\n",
    "                    class_labels = torch.zeros_like(class_labels).long()\n",
    "                noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
    "                loss_target_domain = F.mse_loss(noise_pred, noise)\n",
    "                loss_target_domain.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            if config.class_conditional:\n",
    "                logs = {\"loss\": loss.detach().item(), \"loss_target_domain\": loss_target_domain.detach().item(), \n",
    "                        \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "                writer.add_scalar(\"loss_target_domain\", loss.detach().item(), global_step)\n",
    "            else: \n",
    "                logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "            writer.add_scalar(\"loss\", loss.detach().item(), global_step)\n",
    "\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        if config.model_type == \"DDPM\":\n",
    "            if config.segmentation_guided:\n",
    "                pipeline = SegGuidedDDPMPipeline(\n",
    "                    unet=model.module, scheduler=noise_scheduler, eval_dataloader=eval_dataloader, external_config=config\n",
    "                    )\n",
    "            else:\n",
    "                if config.class_conditional:\n",
    "                    raise NotImplementedError(\"TODO: Conditional training not implemented for non-seg-guided DDPM\")\n",
    "                else:\n",
    "                    pipeline = diffusers.DDPMPipeline(unet=model.module, scheduler=noise_scheduler)\n",
    "        elif config.model_type == \"DDIM\":\n",
    "            if config.segmentation_guided:\n",
    "                pipeline = SegGuidedDDIMPipeline(\n",
    "                    unet=model.module, scheduler=noise_scheduler, eval_dataloader=eval_dataloader, external_config=config\n",
    "                    )\n",
    "            else:\n",
    "                if config.class_conditional:\n",
    "                    raise NotImplementedError(\"TODO: Conditional training not implemented for non-seg-guided DDIM\")\n",
    "                else:\n",
    "                    pipeline = diffusers.DDIMPipeline(unet=model.module, scheduler=noise_scheduler)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            if config.segmentation_guided:\n",
    "                seg_batch = next(eval_dataloader)\n",
    "                evaluate(config, epoch, pipeline, seg_batch)\n",
    "            else:\n",
    "                evaluate(config, epoch, pipeline)\n",
    "\n",
    "        if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:\n",
    "            pipeline.save_pretrained(config.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
